{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import GameHard\n",
    "import DisplayIA\n",
    "\n",
    "env = GameHard.Game()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [15, 2]\n",
    "n_outputs = 3\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\"\"\"\n",
    "K = keras.backend\n",
    "input_states = keras.layers.Input(shape=input_shape)\n",
    "#hidden1 = keras.layers.Flatten()(input_states)\n",
    "#hidden2 = keras.layers.Dense(300, activation=\"relu\")(hidden1)\n",
    "hidden3 = keras.layers.Dense(50, activation=\"relu\")(input_states)\n",
    "hidden4 = keras.layers.Dense(30, activation=\"relu\")(hidden3)\n",
    "hidden5 = keras.layers.Flatten()(hidden4)\n",
    "state_values = keras.layers.Dense(1)(hidden5)\n",
    "raw_advantages = keras.layers.Dense(n_outputs)(hidden5)\n",
    "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
    "Q_values = state_values + advantages\n",
    "model = keras.Model(inputs=[input_states], outputs=[Q_values])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=input_shape),\n",
    "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=keras.initializers.VarianceScaling()),\n",
    "    keras.layers.Dense(200, activation=\"relu\", kernel_initializer=keras.initializers.VarianceScaling()),\n",
    "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=keras.initializers.VarianceScaling()),\n",
    "    keras.layers.Dense(n_outputs, kernel_initializer=keras.initializers.VarianceScaling())\n",
    "])\n",
    "\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_left_count = 0\n",
    "predicted_static_count = 0\n",
    "predicted_right_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    global predicted_static_count, predicted_left_count, predicted_right_count\n",
    "    if np.random.rand() < epsilon:\n",
    "        predicted_value = np.random.randint(3)\n",
    "        return predicted_value\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        predicted_value = np.argmax(Q_values[0])\n",
    "        if predicted_value == 0 :\n",
    "            predicted_static_count += 1\n",
    "        elif predicted_value == 1 :\n",
    "            predicted_left_count += 1\n",
    "        elif predicted_value == 2 :\n",
    "            predicted_right_count += 1\n",
    "        return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "#replay_buffer = deque(maxlen=100000)\n",
    "\n",
    "priority_scale = 0.7\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen=100000)\n",
    "        self.priorities = deque(maxlen=100000)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(np.amax(self.priorities, initial=1))\n",
    "        \n",
    "    def get_probabilities(self, priority_scale):\n",
    "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "    \n",
    "    def get_importance(self, probabilities):\n",
    "        importance = 1/len(self.buffer) * 1/probabilities\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        #indices = np.random.randint(len(self.buffer), size=batch_size)\n",
    "        sample_probs = self.get_probabilities(priority_scale)\n",
    "        indices = np.random.choice(range(len(self.buffer)), batch_size, p=sample_probs)\n",
    "        batch = [self.buffer[index] for index in indices]\n",
    "        states, actions, rewards, next_states, dones = [\n",
    "            np.array([experience[field_index] for experience in batch])\n",
    "            for field_index in range(5)]\n",
    "        importances = self.get_importance(sample_probs[indices])\n",
    "        return indices, states, actions, rewards, next_states, dones, importances\n",
    "    \n",
    "    def set_priorities(self, indices, errors):\n",
    "        for i, e in zip(indices, errors):\n",
    "            self.priorities[i] = abs(e) + 0.1\n",
    "    \n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    cumulative_reward = 0\n",
    "    cumulative_done = False\n",
    "    next_state = None\n",
    "    for i in range(4):\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "        if (done):\n",
    "            cumulative_done = True\n",
    "            break\n",
    "    next_state = np.divide(next_state, 1000)\n",
    "    replay_buffer.append((state, action, cumulative_reward, next_state, cumulative_done))\n",
    "    return next_state, cumulative_reward, cumulative_done, info\n",
    "    \n",
    "    # one frame version\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = np.divide(next_state, 1000)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, action\"\"\"\n",
    "\n",
    "def play_one_step(env, state, epsilon, action=None):\n",
    "    if action == None :\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = next_state.astype(np.float64)\n",
    "    next_state = np.divide(next_state, 1000)\n",
    "    replay_buffer.add((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "discount_factor = 0.99\n",
    "optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0, epsilon=0.00001, centered=True)\n",
    "loss_fn = keras.losses.Huber(reduction=\"none\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model.compile(loss=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch_size):\n",
    "    experiences = replay_buffer.sample(batch_size)\n",
    "    states, actions, rewards, next_states, dones, importances = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    print(next_Q_values)\n",
    "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
    "    print(best_next_actions)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
    "    target_Q_values = rewards + (1 - dones) * discount_factor * next_best_Q_values\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        mask = tf.cast(mask, tf.float64)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "def training(batch_size):\n",
    "    experiences = replay_buffer.sample(batch_size)\n",
    "    indices, states, actions, rewards, next_states, dones, importances = experiences\n",
    "    value_next = np.max(target.predict(next_states), axis=1)\n",
    "    actual_values = np.where(dones, rewards, rewards+discount_factor*value_next)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        mask = tf.one_hot(actions, n_outputs)\n",
    "        mask = tf.cast(mask, tf.float64)\n",
    "        selected_action_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        #selected_action_values = tf.math.reduce_sum(model.predict(states) * tf.one_hot(actions, n_outputs), keepdims=True)\n",
    "        error = actual_values - selected_action_values\n",
    "        replay_buffer.set_priorities(indices, error)\n",
    "        loss = tf.math.reduce_mean(tf.multiply(tf.square(error), importances))\n",
    "    variables = model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Play random for a few episodes\n",
    "for episode in range(1000):\n",
    "    obs = env.reset()\n",
    "    obs = np.divide(obs, 1000)\n",
    "    \n",
    "    for step in range(5000):\n",
    "        obs, rewards, done, info = play_one_step(env, obs, 1, None)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if episode % 500 == 0:\n",
    "        print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step number : 0\n",
      "episode number : 0\n",
      "predicted static : 0\n",
      "predicted left : 0\n",
      "predicted right : 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Trying something else\n",
    "\n",
    "#Init variable\n",
    "obs = env.reset()\n",
    "obs = obs.astype(np.float64)\n",
    "obs = np.divide(obs, 1000)\n",
    "\n",
    "episode = 0\n",
    "total_rewards = []\n",
    "\n",
    "episode_total_rewards = 0\n",
    "\n",
    "repeat = 0\n",
    "action_to_repeat = 0\n",
    "for step in range(500000):\n",
    "    epsilon = max(1 - (step / 150000), 0.01)\n",
    "    \n",
    "    if repeat == 0 :\n",
    "        obs, rewards, done, info = play_one_step(env, obs, epsilon, None)\n",
    "        repeat = 3\n",
    "        action_to_repeat = info\n",
    "    else :\n",
    "        obs, rewards, done, info = play_one_step(env, obs, epsilon, action_to_repeat)\n",
    "        repeat -= 1\n",
    "    episode_total_rewards += rewards\n",
    "    \n",
    "    if done :\n",
    "        obs = env.reset()\n",
    "        obs = np.divide(obs, 1000)\n",
    "        episode += 1\n",
    "        total_rewards = np.append(total_rewards, episode_total_rewards)\n",
    "        episode_total_rewards = 0\n",
    "    \n",
    "    if step % 4 == 0 :\n",
    "        training(batch_size)\n",
    "    if step % 2000 == 0:\n",
    "        target.set_weights(model.get_weights())\n",
    "    \n",
    "    if step % 10000 == 0 :\n",
    "        print(\"step number : \" + str(step))\n",
    "        print(\"episode number : \" + str(episode))\n",
    "        \n",
    "        print(\"predicted static : \" + str(predicted_static_count))\n",
    "        print(\"predicted left : \" + str(predicted_left_count))\n",
    "        print(\"predicted right : \" + str(predicted_right_count))\n",
    "        \n",
    "        random_static_count = 0\n",
    "        random_left_count = 0\n",
    "        random_right_count = 0\n",
    "        predicted_static_count = 0\n",
    "        predicted_left_count = 0\n",
    "        predicted_right_count = 0\n",
    "        \n",
    "        scores = []\n",
    "        model.save('kerasDQL.h5')\n",
    "        plt.plot(total_rewards)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"total_rewards = []\n",
    "\n",
    "scores = []\n",
    "for episode in range(30000):       \n",
    "\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = np.divide(obs, 1000)\n",
    "\n",
    "    episode_total_rewards = 0\n",
    "\n",
    "    for step in range(50000):\n",
    "        epsilon = max(1 - episode / 7500, 0.01)\n",
    "        obs, rewards, done, info = play_one_step(env, obs, epsilon)\n",
    "        episode_total_rewards += rewards\n",
    "        if done:\n",
    "            scores = np.append(scores, episode_total_rewards)\n",
    "            total_rewards = np.append(total_rewards, episode_total_rewards)\n",
    "            break\n",
    "        if episode > 100 and episode < 3000 and step % 2000 == 0:\n",
    "            training_step(batch_size)\n",
    "    if episode % 50 == 0 :\n",
    "        target.set_weights(model.get_weights())\n",
    "    if episode % 500 == 0 :\n",
    "        print(\"episode number : \" + str(episode))\n",
    "        print(\"mean score : \" + str(np.mean(scores)))\n",
    "        \n",
    "        print(\"random static : \" + str(random_static_count))\n",
    "        print(\"random left : \" + str(random_left_count))\n",
    "        print(\"random right : \" + str(random_right_count))\n",
    "        print(\"predicted static : \" + str(predicted_static_count))\n",
    "        print(\"predicted left : \" + str(predicted_left_count))\n",
    "        print(\"predicted right : \" + str(predicted_right_count))\n",
    "        \n",
    "        random_static_count = 0\n",
    "        random_left_count = 0\n",
    "        random_right_count = 0\n",
    "        predicted_static_count = 0\n",
    "        predicted_left_count = 0\n",
    "        predicted_right_count = 0\n",
    "        \n",
    "        scores = []\n",
    "        model.save('kerasDQL.h5')\n",
    "        plt.plot(total_rewards)\n",
    "        plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('kerasDQL.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "250000 // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
